{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f259a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 10:28:54.537088: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-25 10:28:54.537130: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot;\n",
    "import tensorflow as tf;\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10b8d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Data Loading ================\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================ Data Loading ================\")\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "x_train = np.reshape(x_train, (-1,28, 28, 1))\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb9e47",
   "metadata": {},
   "source": [
    "## Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380c7c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 10:29:01.597554: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-25 10:29:01.597608: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-25 10:29:01.597637: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big25): /proc/driver/nvidia/version does not exist\n",
      "2022-05-25 10:29:01.599285: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc41c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karimhocine/.local/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2994 - sparse_categorical_accuracy: 0.9099 - val_loss: 0.0930 - val_sparse_categorical_accuracy: 0.9709\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.0820 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.0697 - val_sparse_categorical_accuracy: 0.9789\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0586 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0519 - val_sparse_categorical_accuracy: 0.9830\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9860 - val_loss: 0.0407 - val_sparse_categorical_accuracy: 0.9868\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0381 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0393 - val_sparse_categorical_accuracy: 0.9872\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0324 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0378 - val_sparse_categorical_accuracy: 0.9881\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0286 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.0396 - val_sparse_categorical_accuracy: 0.9887\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9922 - val_loss: 0.0410 - val_sparse_categorical_accuracy: 0.9869\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0326 - val_sparse_categorical_accuracy: 0.9901\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9934 - val_loss: 0.0325 - val_sparse_categorical_accuracy: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff6df5dd00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data= (x_test, y_test),\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ff5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantifie = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b88da5",
   "metadata": {},
   "source": [
    "## Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "308c0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  matrix2vect(w):\n",
    "    # Vectorisation de la matrice des poids\n",
    "    layer_shapes = []\n",
    "    vect = []\n",
    "\n",
    "    for i in range (len(w)//2):\n",
    "      # rassembler les poids et biais dans une seule matrices\n",
    "      v = tf.concat(  [tf.reshape(w[2*i],( -1,1 )), tf.reshape(w[2*i+1], (-1,1)) ], axis=0)\n",
    "\n",
    "      # enregistrer les dimensiosn des matrices pour effectuer l'opération inverse\n",
    "      layer_shapes.append(v.shape)\n",
    "\n",
    "      # Vectoriser les matrices de poids-biais\n",
    "      vect.append(tf.reshape(v, (-1, 1)))\n",
    "    \n",
    "    return vect, layer_shapes\n",
    "                                               \n",
    "def vect2matrix(self, v):\n",
    "    w = []\n",
    "    # Cette fonction permet de transformer les poids quantifiées sous le formats correspondants aux dimensions des poids des couches\n",
    "    for i in range(len(v)):\n",
    "        mat = tf.reshape(v[i], self.layer_shapes[i])\n",
    "        w.append(mat[0:-1])\n",
    "        w.append(mat[-1])\n",
    "    return w\n",
    "\n",
    "\n",
    "  # Transforme les valeurs d'un vecteur vers l'intervalle [0, 1]\n",
    "def scale_function(tab, bucket_size):\n",
    "\n",
    "  Vecteur = []\n",
    "  A = []\n",
    "  B = []\n",
    "\n",
    "  for tab in tab:\n",
    "        tab = tf.reshape(tab, (-1))\n",
    "        if bucket_size > tab.shape[0]:\n",
    "          raise ValueError(f'Bucket_size ({bucket_size}) must be smaller than or equal to the vector length ({len(tab)})')\n",
    "        v = tf.constant([])\n",
    "        alpha = []\n",
    "        beta = []\n",
    "\n",
    "        nb_bucket = tab.shape[0]//bucket_size\n",
    "\n",
    "        # Nombre de bucket pair\n",
    "        if tab.shape[0] % bucket_size == 0:\n",
    "          nb_param = nb_bucket\n",
    "\n",
    "\n",
    "          for i in range(nb_param):\n",
    "            b = tf.math.reduce_min(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "            a = tf.math.reduce_max(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "            alpha.append(a)\n",
    "            beta.append(b)\n",
    "\n",
    "            \"\"\"if a == 0:\n",
    "                  a = tf.constant([1])\"\"\"\n",
    "            vect = (tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) - b) / a\n",
    "            v = tf.concat([v, vect], 0)\n",
    "\n",
    "        # Nombre de bucket impair\n",
    "        else:\n",
    "          nb_param = nb_bucket + 1\n",
    "\n",
    "          for i in range(nb_param):\n",
    "            if i == nb_param - 1:\n",
    "              b = tf.math.reduce_min(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) )\n",
    "              a = tf.math.reduce_max(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]) ) )\n",
    "              alpha.append(a)\n",
    "              beta.append(b)\n",
    "\n",
    "              \"\"\"if a == 0:\n",
    "                  a = tf.constant([1])\"\"\"\n",
    "\n",
    "              vect = (  tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([tab.shape[0] - i* bucket_size]))  - b) /(a)\n",
    "\n",
    "            else:\n",
    "              b = tf.math.reduce_min(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "              a = tf.math.reduce_max(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) ) - tf.math.reduce_min(tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) )\n",
    "              alpha.append(a)\n",
    "              beta.append(b)\n",
    "\n",
    "              \"\"\"if a == 0:\n",
    "                  a = tf.constant([1])\"\"\"\n",
    "\n",
    "              vect = (tf.slice(tab[:], begin=tf.constant([i* bucket_size]), size= tf.constant([bucket_size]) ) - b) / a\n",
    "            v = tf.concat([v, vect], 0)\n",
    "\n",
    "        A.append(alpha)\n",
    "        B.append(beta)\n",
    "        Vecteur.append(v) \n",
    "\n",
    "  return Vecteur, A, B\n",
    "\n",
    "def uniform_quantification( Vect, nb_bits):\n",
    "    v_q = []\n",
    "    s = tf.constant([2**nb_bits], dtype=tf.float32)\n",
    "\n",
    "    for v in Vect:\n",
    "        k = tf.math.subtract(tf.math.multiply(v,s) , tf.math.floor( tf.math.multiply(v,s)) )\n",
    "        eps = tf.ones(k.shape[0])\n",
    "\n",
    "        eps = tf.where( tf.greater(k,0.5),eps, 0 )\n",
    "        v_s = tf.math.multiply(v,s)\n",
    "        res =  tf.math.floor(tf.math.divide(v_s, s))\n",
    "\n",
    "        Q =  res + (tf.math.divide(eps ,s) )\n",
    "\n",
    "        v_q.append(Q)\n",
    "    return v_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb57362",
   "metadata": {},
   "source": [
    "### Transformer les poids en vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd8b621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 6)\n",
      "(6,)\n",
      "(5, 5, 6, 16)\n",
      "(16,)\n",
      "(256, 120)\n",
      "(120,)\n",
      "(120, 84)\n",
      "(84,)\n",
      "(84, 10)\n",
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TensorShape([156, 1]),\n",
       " TensorShape([2416, 1]),\n",
       " TensorShape([30840, 1]),\n",
       " TensorShape([10164, 1]),\n",
       " TensorShape([850, 1])]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = model.weights\n",
    "original_shapes = []\n",
    "for W in w:\n",
    "    print(W.shape)\n",
    "    original_shapes.append(W.shape)\n",
    "    \n",
    "    \n",
    "vect, layer_shapes = matrix2vect(w)\n",
    "layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1c8ce995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.26260594  0.05045616  0.14847375  0.34235522  0.0350243   0.29450238\n",
      " -0.16412902  0.12851956  0.26572266  0.20774849], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01233123  0.04869561  0.03296651  0.0364034   0.08748352 -0.00524808\n",
      " -0.08807354  0.20188634  0.03940699  0.09373177], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.03111342 -0.18106097  0.13798894  0.1205007  -0.05764659 -0.1340255\n",
      " -0.19832337 -0.04008258 -0.05186015 -0.11022102], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 0.16204588 -0.1678768   0.1572961   0.15544422 -0.25573942  0.07330917\n",
      " -0.13756572 -0.06489266  0.21713604 -0.09113318], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.3712538   0.1432257   0.14423928 -0.32695284  0.2935225  -0.05299498\n",
      " -0.15595293  0.1571002   0.06839338  0.20857866], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for v in vect:\n",
    "    v = tf.reshape(v, (-1))\n",
    "    print(tf.slice(v, [0], [10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2ab13",
   "metadata": {},
   "source": [
    "## Normaliser le Vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d383ab39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, alpha, beta = scale_function(vect, bucket_size = 128)\n",
    "len(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b4965",
   "metadata": {},
   "source": [
    "### Effectuer une quantification uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "85eab799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(156,), dtype=float32, numpy=\n",
       " array([0.00390625, 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        1.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2416,), dtype=float32, numpy=\n",
       " array([0.        , 0.        , 0.        , ..., 0.00390625, 0.00390625,\n",
       "        0.        ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(30840,), dtype=float32, numpy=\n",
       " array([0.        , 0.        , 0.        , ..., 0.00390625, 0.        ,\n",
       "        0.        ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10164,), dtype=float32, numpy=\n",
       " array([0.        , 0.00390625, 0.00390625, ..., 0.        , 0.        ,\n",
       "        0.00390625], dtype=float32)>,\n",
       " <tf.Tensor: shape=(850,), dtype=float32, numpy=\n",
       " array([0.        , 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        1.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 1.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 1.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.00390625, 0.        , 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.        , 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.        ,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.        , 0.00390625, 0.00390625, 0.        , 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.        , 1.        , 0.        ,\n",
       "        0.00390625, 0.00390625, 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.00390625, 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.00390625, 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.        , 0.        , 0.        , 0.00390625,\n",
       "        0.        , 0.        , 0.00390625, 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.        , 0.        , 0.        , 0.00390625, 0.00390625,\n",
       "        0.00390625, 0.00390625, 0.00390625, 0.        , 0.00390625],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = uniform_quantification(v, 8)\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ee246",
   "metadata": {},
   "source": [
    "### Repasser les poids sous forme matricielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074a516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e7a947",
   "metadata": {},
   "source": [
    "### Appliquer les poids au model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6a76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6bf6ca",
   "metadata": {},
   "source": [
    "### Evaluer les performances du model quantifié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9c558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f7967812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255, 255, 255], dtype=uint8)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "t = np.ones(100, dtype = \"uint8\")*1000\n",
    "tab = (t/10).astype(\"uint8\")\n",
    "tab - 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8c84a884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4e7cea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.astype(\"float32\")\n",
    "getsizeof(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0729c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
